{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19lZTFKzGIuH"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import v2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "# from sklearn.metrics import classification_report\n",
        "import time\n",
        "\n",
        "####\n",
        "import json\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5jQV3i6mOqu",
        "outputId": "802a31cf-ce23-4516-a9e2-1194a1797635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the metadata from the JSON file\n",
        "with open('WLASL_v0.3.json', 'r') as file:\n",
        "  metadata = json.load(file)"
      ],
      "metadata": {
        "id": "m8kmtCnfHJy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/videos/\"\n",
        "file_extension = \".mp4\"\n",
        "# Define a function to check if the file exists\n",
        "def check_file_exists(row):\n",
        "  file_name = f\"{row['video_id']}{file_extension}\"\n",
        "  file_path = os.path.join(folder_path, file_name)\n",
        "  return os.path.exists(file_path)"
      ],
      "metadata": {
        "id": "E-ytUT-VLhG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels = {'a': 1281, 'b': 1315, 'c': None, 'd': 1693, 'e': 1726, 'f': 1742, 'g': 1757, 'h': 1768, 'i': 1779,\n",
        "#           'j': 1789, 'k': 1794, 'l': None, 'm': 1814, 'n': 1829, 'o': 1840, 'p': 1471, 'q': 1892, 'r': 1896,\n",
        "#           's': 1917, 't': 1959, 'u': 1980, 'v': 1983, 'w': 1994, 'x': None, 'y': None, 'z': None}\n",
        "labels = {'a': 0, 'b': 1, 'c': None, 'd': 2, 'e': 3, 'f': 4, 'g': 5, 'h': 6, 'i': 7,\n",
        "          'j': 8, 'k': 9, 'l': None, 'm': 10, 'n': 11, 'o': 12, 'p': 13, 'q': 14, 'r': 15,\n",
        "          's': 16, 't': 17, 'u': 18, 'v': 19, 'w': 20, 'x': None, 'y': None, 'z': None}\n",
        "len_labels = len(labels)\n",
        "print(len_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NOLrGEr_qcZ",
        "outputId": "c620efec-baee-46e2-c102-63c4f49595f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "for header in metadata:\n",
        "  letter = header['gloss']\n",
        "  if letter in ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']:\n",
        "    temp = pd.json_normalize(header, record_path=['instances'], meta=['gloss']).drop(['source', 'variation_id', 'url', 'signer_id', 'instance_id'], axis=1)\n",
        "    temp['labels'] = labels[letter]\n",
        "    # Remove rows that has missing video ids\n",
        "    mask = temp.apply(check_file_exists, axis=1)  ## axis 1 is row\n",
        "    temp = temp[mask]\n",
        "    # Concat the dataframes\n",
        "    df = pd.concat([df, temp])\n",
        "\n",
        "print(df)\n",
        "#print(df.iloc[0])\n",
        "# This dataset has 21 letters only\n",
        "# bbox column_num = 0,....etc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBLvVsl5iV4h",
        "outputId": "2a02edf9-c5a9-420e-f061-b561ffeba0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   bbox  fps  frame_end  frame_start  split video_id gloss  \\\n",
            "1   [167, 13, 485, 370]   25         -1            1   test    66039     a   \n",
            "2   [124, 33, 565, 480]   25         -1            1  train    01610     a   \n",
            "4     [85, 7, 217, 192]   25         -1            1  train    01612     a   \n",
            "7   [179, 44, 586, 400]   25         -1            1  train    01615     a   \n",
            "0   [204, 31, 512, 370]   25         -1            1    val    66040     b   \n",
            "..                  ...  ...        ...          ...    ...      ...   ...   \n",
            "6   [174, 48, 559, 400]   25         -1            1  train    61565     v   \n",
            "1   [201, 27, 511, 370]   25         -1            1    val    66061     w   \n",
            "2   [129, 39, 581, 478]   25         -1            1  train    63274     w   \n",
            "4     [84, 9, 220, 192]   25         -1            1  train    63276     w   \n",
            "6   [228, 34, 485, 400]   25         -1            1  train    63278     w   \n",
            "\n",
            "    labels  \n",
            "1        0  \n",
            "2        0  \n",
            "4        0  \n",
            "7        0  \n",
            "0        1  \n",
            "..     ...  \n",
            "6       19  \n",
            "1       20  \n",
            "2       20  \n",
            "4       20  \n",
            "6       20  \n",
            "\n",
            "[84 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[1]['video_id'])\n",
        "metadata = df.iloc[1]\n",
        "label = metadata['labels']\n",
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwkukKbLvlqI",
        "outputId": "864f2256-681a-4f39-f09d-6ffc44ab695a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01610\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## TEST #############\n",
        "# Testing Image Snippet\n",
        "bbox = df.iloc[1]['bbox']\n",
        "fps = 30\n",
        "frame_end = df.iloc[1]['frame_end']\n",
        "frame_start = df.iloc[1]['frame_start']\n",
        "video_path = f\"/content/drive/MyDrive/videos/{df.iloc[1]['video_id']}.mp4\"\n",
        "frames = preprocess_videos(bbox, fps, frame_end, frame_start, video_path)\n",
        "\n",
        "delay = 1 / fps\n",
        "\n",
        "for frame in frames:\n",
        "  # Display the frame\n",
        "  cv2_imshow(frame)\n",
        "\n",
        "  # Wait for the specified delay or until a key is pressed\n",
        "  if cv2.waitKey(int(delay * 1000)) & 0xFF == ord('q'):  # Press 'q' to quit\n",
        "      break\n",
        "\n",
        "# Close the window after playback is finished\n",
        "cv2.destroyAllWindows()\n",
        "print(len(frames))"
      ],
      "metadata": {
        "id": "B6RWfdgJvZtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  ## No need for cuda:0 here as it only has one GPU and 0 is default\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O-w-FvoS72R",
        "outputId": "7fbec467-a01a-4794-ad52-d268a9a68049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT COMPUTING MEAN AND STD HERE"
      ],
      "metadata": {
        "id": "pLnyg3-7TJTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations to apply to each frame\n",
        "transformations = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),  # Scale to range [0, 1]\n",
        "    # v2.Normalize(mean=[mean], std=[std])  # Not doing mean and std for dataset\n",
        "])"
      ],
      "metadata": {
        "id": "hgaN5YDtX5G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_videos(bbox, fps, frame_end, frame_start, video_path):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "  duration_frames = 1 * 30  # 2 sec, 30 fps\n",
        "\n",
        "  if not cap.isOpened():\n",
        "    print(\"[INFO] Warning: Could not open video.\" + video_path)\n",
        "    return\n",
        "\n",
        "  frames = []\n",
        "  current_frame = 1\n",
        "\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    if (current_frame >= frame_start) and (frame_end == -1 or current_frame <= frame_end):\n",
        "      # Apply bounding box\n",
        "      x_min, y_min, x_max, y_max = bbox\n",
        "      cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "      # Resize frame\n",
        "      output_frame_size = (150, 150)  # Arbitrary\n",
        "      resized_frame = cv2.resize(cropped_frame, output_frame_size)  ## I can specify the interpolation method for better resizing such as BiCubic, Bilinear, etc.\n",
        "\n",
        "      # Randomly flipping, rotating frames\n",
        "      if random.random() > 0.5:\n",
        "        resized_frame = cv2.flip(resized_frame, 1)  # 1 means flipping around y-axis\n",
        "\n",
        "      # Randomly rotate the frame\n",
        "      rotation_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE]\n",
        "      rotation = random.choice(rotation_choices)\n",
        "      if rotation != 0:\n",
        "          resized_frame = cv2.rotate(resized_frame, rotation)\n",
        "\n",
        "      # Convert cv2 BGR to RGB format\n",
        "      resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      # Transpose the frame from [height, width, channels] to [channels, height, width]\n",
        "      resized_frame = np.transpose(resized_frame, (2, 0, 1))  #This format is PyTorch Native\n",
        "\n",
        "      # Tensor casting\n",
        "      tensor_frame = torch.from_numpy(resized_frame)  # Change numpy array to tensor # No need for .float() here as I am using transform above\n",
        "      frame_transformed = transformations(tensor_frame)  # Apply transformations\n",
        "\n",
        "      frames.append(frame_transformed)\n",
        "\n",
        "    current_frame += 1\n",
        "\n",
        "  cap.release()\n",
        "\n",
        "  if len(frames) < duration_frames:\n",
        "    # If fewer frames than desired, repeat the last frame\n",
        "    frames += [frames[-1]] * (duration_frames - len(frames))\n",
        "  elif len(frames) > duration_frames:\n",
        "    # If more frames than desired, sample evenly from the extracted frames\n",
        "    indices = torch.linspace(0, len(frames) - 1, duration_frames, dtype=torch.int)\n",
        "    frames = [frames[i] for i in indices]\n",
        "\n",
        "  return frames"
      ],
      "metadata": {
        "id": "_31H4GdP2E72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoSignLanguageDataset(Dataset):\n",
        "  def __init__(self, df, root):\n",
        "    self.data_frame = df\n",
        "    # self.transform = transform\n",
        "    self.root_path = root\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_frame)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    metadata = self.data_frame.iloc[idx]\n",
        "    bbox = metadata['bbox']\n",
        "    fps = metadata['fps']\n",
        "    frame_end = metadata['frame_end']\n",
        "    frame_start = metadata['frame_start']\n",
        "    split = metadata['split']\n",
        "    video_id = metadata['video_id']\n",
        "    gloss = metadata['gloss']\n",
        "    # label = self.data_frame.iloc[idx, -1]\n",
        "    label = metadata['labels']\n",
        "    video_path = f'{self.root_path}/{video_id}.mp4'\n",
        "\n",
        "    frames = preprocess_videos(bbox, fps, frame_end, frame_start, video_path)\n",
        "\n",
        "    # label = torch.tensor(label).long()  ## Both lines are almost same\n",
        "    label = torch.tensor(label, dtype=torch.int64)\n",
        "\n",
        "    if frames:\n",
        "      # Ensure all frames are of the same shape and tensor type before stacking\n",
        "      tensor_frames = torch.stack(frames)\n",
        "\n",
        "      # if self.transform:\n",
        "      #   frames = self.transform(frames)\n",
        "      # print(\"tf_type = \", tensor_frames.dtype)\n",
        "      # print(\"tf_size = \", tensor_frames.size())\n",
        "      return tensor_frames, label"
      ],
      "metadata": {
        "id": "9wo-6x2H5oWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom collate function to remove None values\n",
        "def custom_collate_fn(batch):\n",
        "  batch = list(filter(lambda x: x is not None, batch))  # Filter out None values\n",
        "\n",
        "  if not batch:\n",
        "    # Return None or a custom signal that indicates an empty batch\n",
        "    return None  ## This return None is not very good, I have to handle this better so it returns something default_collate() can work with.\n",
        "\n",
        "  return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "BATCH_SIZE = 12\n",
        "ROOT = \"/content/drive/MyDrive/videos\"\n",
        "training_dataset = VideoSignLanguageDataset(df=df, root=ROOT)\n",
        "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "OvEpRJxMTiL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_batch)\n",
        "unique_tensor = torch.unique(y_batch, return_counts=True)\n",
        "print(unique_tensor, len(unique_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CRY2PQNbzPo",
        "outputId": "148b5931-72e1-45f4-a6ce-b43455464ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1315, 1980, 1814, 1840, 1829, 1896, 1768, 1726, 1789, 1757, 1959, 1896,\n",
            "        1840, 1768, 1892, 1281, 1917, 1789, 1471, 1742, 1757, 1829, 1281, 1994,\n",
            "        1980, 1959, 1693, 1726, 1794, 1779, 1794, 1917, 1779, 1959, 1471, 1980,\n",
            "        1789, 1917, 1768, 1814, 1757, 1840, 1757, 1980, 1281, 1829, 1892, 1896,\n",
            "        1814, 1281, 1983, 1742, 1693, 1959, 1693, 1768, 1726, 1779, 1814, 1994,\n",
            "        1471, 1983, 1994, 1917, 1983, 1726, 1892, 1693, 1892, 1742, 1994, 1829,\n",
            "        1840, 1779, 1983, 1315, 1794, 1896, 1794, 1471, 1315, 1742, 1315, 1789])\n",
            "(tensor([1281, 1315, 1471, 1693, 1726, 1742, 1757, 1768, 1779, 1789, 1794, 1814,\n",
            "        1829, 1840, 1892, 1896, 1917, 1959, 1980, 1983, 1994]), tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])) 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN + LSTM Model"
      ],
      "metadata": {
        "id": "qRddfeApdEwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLSTMNet(nn.Module):\n",
        "  def __init__(self, input_channels, output_classes):\n",
        "    super().__init__()\n",
        "    self.cnn_model = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_channels, out_channels=96, kernel_size=(6, 6)),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3)),  ## the dimensions of the input feature map should be multiples of the stride for better performance so the stride wont miss some corner parts of the feature map grid\n",
        "        nn.Conv2d(in_channels=96, out_channels=192, kernel_size=(5, 5)),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3)),\n",
        "        nn.Conv2d(in_channels=192, out_channels=384, kernel_size=(4, 4)),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),  ## stride can be none as it defaults to kernel_size when none\n",
        "        nn.Conv2d(in_channels=384, out_channels=768, kernel_size=(3, 3)),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),  ## stride can be none as it defaults to kernel_size when none\n",
        "        nn.AdaptiveAvgPool2d((1, 1))\n",
        "    )\n",
        "\n",
        "    # This snippet is for programatically determining the cnn output to serve as input for LSTM\n",
        "    dummy_input = torch.randn(1, input_channels, 150, 150)  ## [batch_size, channels, height, width]\n",
        "    dummy_output = self.cnn_model(dummy_input)\n",
        "    cnn_output = dummy_output.view(dummy_output.size(0), -1).size(1)\n",
        "    #cnn_output = dummy_output.size(1)\n",
        "\n",
        "    # LSTM Parameters\n",
        "    self.output_classes = output_classes  # number of output classes # 21 is num of alphabets available\n",
        "    self.num_layers = 2  # number of stacked layers of LSTM\n",
        "    self.input_size = cnn_output\n",
        "    self.hidden_size = 512  # Arbitrary\n",
        "    # self.seq_length = 60  # Frames length ## Maybe use dummy_output.size(1)\n",
        "\n",
        "    self.LSTM = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
        "    self.fc_1 = nn.Linear(in_features=self.hidden_size, out_features=256)\n",
        "    self.relu_1 = nn.ReLU()\n",
        "    self.fc_2 = nn.Linear(in_features=256, out_features=self.output_classes)\n",
        "    # No need for any final activation here if I'm using CrossEntropyLoss which internally handles 'Softmax' + 'NLLLoss' in single operation\n",
        "\n",
        "  def forward(self, x):\n",
        "    print(f\"Original x shape: {x.shape}\")  # Debugging\n",
        "    batch_size, seq_length, c, h, w = x.size()\n",
        "\n",
        "    # x = x.view(batch_size * seq_length, c, h, w)  ## Maybe better alternative\n",
        "    # x = x.view(-1, x.size(2), x.size(3), x.size(4))\n",
        "    x = x.view(-1, c, h, w) ## Converting 5D tensor to 4D as CNN expects\n",
        "    print(f\"Reshaped x for CNN shape: {x.shape}\")  # Debugging\n",
        "    x = self.cnn_model(x)  ## Feeding into CNN\n",
        "\n",
        "    print(f\"Output from CNN shape: {x.shape}\")  # Debugging\n",
        "\n",
        "    # Reshape CNN output for LSTM input\n",
        "    x = x.view(batch_size, seq_length, -1)  ## [batch_size, frames_of_video, features]\n",
        "    print(f\"Reshaped x for LSTM shape: {x.shape}\")  # Debugging\n",
        "    lstm_out, (hn, cn) = self.LSTM(x)  ## Feed into LSTM\n",
        "\n",
        "    # For many-to-one tasks, you might want to use hn[-1] or lstm_out[:, -1, :]\n",
        "    x = lstm_out[:, -1, :]\n",
        "\n",
        "    # Feed LSTM 'extracted' output to the following dense layers\n",
        "    x = self.fc_1(x)\n",
        "    x = self.relu_1(x)\n",
        "    x = self.fc_2(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "_n-pFiT0o5e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reproducible RNGs"
      ],
      "metadata": {
        "id": "m8X0AeNkdPiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set RNGs to same values every time including CUDA operations\n",
        "seed = 10\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "#torch.cuda.manual_seed_all(seed) ## For multiple GPUs"
      ],
      "metadata": {
        "id": "LlSiMWbadO1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & Testing"
      ],
      "metadata": {
        "id": "i4CRfeMRsCkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating instance of model\n",
        "input_channels = 3  # RGB\n",
        "output_classes = 21  # ASL # 21 is num of alphabets available in the dataset\n",
        "conv_lstm_model = ConvLSTMNet(input_channels, output_classes).to(DEVICE)\n",
        "print(conv_lstm_model)\n",
        "print(next(conv_lstm_model.parameters()).device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kONVEHYd67r",
        "outputId": "879d0151-5e73-4737-d7c3-114fcf5d72b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvLSTMNet(\n",
            "  (cnn_model): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(6, 6), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(4, 4), stride=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (LSTM): LSTM(768, 512, num_layers=2, batch_first=True)\n",
            "  (fc_1): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (relu_1): ReLU()\n",
            "  (fc_2): Linear(in_features=256, out_features=21, bias=True)\n",
            ")\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and Optimizer\n",
        "LRN_RATE = 0.01\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "g_descent_optimizer = torch.optim.Adam(conv_lstm_model.parameters(), lr=LRN_RATE)  ##Adam is a type of gradient descent"
      ],
      "metadata": {
        "id": "FusGy-Oiszym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Model\n",
        "overall_train_losses = []\n",
        "overall_train_accuracy = []\n",
        "#test_losses = []\n",
        "#test_correct = []\n",
        "\n",
        "EPOCHS = 6\n",
        "#total_dataset = len(train_dataloader.dataset)\n",
        "\n",
        "print(\"[INFO] Training the network...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "  conv_lstm_model.train()  ## Set model to train mode\n",
        "\n",
        "  # These variables are for entire dataset once (all batches)\n",
        "  total_train_loss = 0\n",
        "  total_train_correct = 0\n",
        "\n",
        "  for X_batch, y_batch in train_dataloader:\n",
        "    #(X_batch[0].view(28, 28)\n",
        "    #print(X_batch.dim())  ## 4-Dimensions\n",
        "    #print(X_batch[0].dim())  ## 3-D\n",
        "\n",
        "    # Push Data Tensors to the GPU\n",
        "    (X_batch, y_batch) = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "\n",
        "    pred = conv_lstm_model(X_batch)  ## Predicted values of y\n",
        "    loss = loss_function(pred, y_batch)\n",
        "\n",
        "    g_descent_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    g_descent_optimizer.step()\n",
        "\n",
        "    total_train_loss += loss\n",
        "    total_train_correct += (pred.argmax(1) == y_batch).type(torch.float).sum().item()\n",
        "\n",
        "  ## For one epoch\n",
        "  avg_train_accuracy = total_train_correct / len(train_dataloader.dataset)\n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "  overall_train_accuracy.append(avg_train_accuracy)\n",
        "  overall_train_losses.append(avg_train_loss)\n",
        "\n",
        "  print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
        "  print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avg_train_loss, avg_train_accuracy))\n",
        "  print(f'{total_train_correct}/{len(train_dataloader.dataset)}')\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f'Total Training Time: {total_time/60} Minutes.')"
      ],
      "metadata": {
        "id": "LN-kzkl4tyh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}