{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19lZTFKzGIuH"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import v2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "# from sklearn.metrics import classification_report\n",
        "import time\n",
        "from torchvision import models\n",
        "\n",
        "####\n",
        "import json\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "# import tensorflow as tf\n",
        "# import keras\n",
        "# import tensorflow.keras.layers as tkl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5jQV3i6mOqu",
        "outputId": "f8f842c7-cd6b-4c5f-bfec-ee55052cc1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the metadata from the JSON file\n",
        "with open('WLASL_v0.3.json', 'r') as file:\n",
        "  metadata = json.load(file)"
      ],
      "metadata": {
        "id": "m8kmtCnfHJy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/videos/\"\n",
        "file_extension = \".mp4\"\n",
        "# Define a function to check if the file exists\n",
        "def check_file_exists(row):\n",
        "  file_name = f\"{row['video_id']}{file_extension}\"\n",
        "  file_path = os.path.join(folder_path, file_name)\n",
        "  return os.path.exists(file_path)"
      ],
      "metadata": {
        "id": "E-ytUT-VLhG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels = {'a': 1281, 'b': 1315, 'c': None, 'd': 1693, 'e': 1726, 'f': 1742, 'g': 1757, 'h': 1768, 'i': 1779,\n",
        "#           'j': 1789, 'k': 1794, 'l': None, 'm': 1814, 'n': 1829, 'o': 1840, 'p': 1471, 'q': 1892, 'r': 1896,\n",
        "#           's': 1917, 't': 1959, 'u': 1980, 'v': 1983, 'w': 1994, 'x': None, 'y': None, 'z': None}\n",
        "labels = {'a': 0, 'b': 1, 'c': None, 'd': 2, 'e': 3, 'f': 4, 'g': 5, 'h': 6, 'i': 7,\n",
        "          'j': 8, 'k': 9, 'l': None, 'm': 10, 'n': 11, 'o': 12, 'p': 13, 'q': 14, 'r': 15,\n",
        "          's': 16, 't': 17, 'u': 18, 'v': 19, 'w': 20, 'x': None, 'y': None, 'z': None}\n",
        "len_labels = len(labels)\n",
        "print(len_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NOLrGEr_qcZ",
        "outputId": "4e01960d-1d8b-4ea4-9c18-aebc61b3e1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "for header in metadata:\n",
        "  letter = header['gloss']\n",
        "  if letter in ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']:\n",
        "    temp = pd.json_normalize(header, record_path=['instances'], meta=['gloss']).drop(['source', 'variation_id', 'url', 'signer_id', 'instance_id'], axis=1)\n",
        "    temp['labels'] = labels[letter]\n",
        "    # Remove rows that has missing video ids\n",
        "    mask = temp.apply(check_file_exists, axis=1)  ## axis 1 is row\n",
        "    temp = temp[mask]\n",
        "    # Concat the dataframes\n",
        "    df = pd.concat([df, temp])\n",
        "\n",
        "print(df)\n",
        "#print(df.iloc[0])\n",
        "# This dataset has 21 letters only\n",
        "# bbox column_num = 0,....etc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBLvVsl5iV4h",
        "outputId": "75146b58-946e-4d3a-d7db-712cf6905e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   bbox  fps  frame_end  frame_start  split video_id gloss  \\\n",
            "1   [167, 13, 485, 370]   25         -1            1   test    66039     a   \n",
            "2   [124, 33, 565, 480]   25         -1            1  train    01610     a   \n",
            "4     [85, 7, 217, 192]   25         -1            1  train    01612     a   \n",
            "7   [179, 44, 586, 400]   25         -1            1  train    01615     a   \n",
            "0   [204, 31, 512, 370]   25         -1            1    val    66040     b   \n",
            "..                  ...  ...        ...          ...    ...      ...   ...   \n",
            "6   [174, 48, 559, 400]   25         -1            1  train    61565     v   \n",
            "1   [201, 27, 511, 370]   25         -1            1    val    66061     w   \n",
            "2   [129, 39, 581, 478]   25         -1            1  train    63274     w   \n",
            "4     [84, 9, 220, 192]   25         -1            1  train    63276     w   \n",
            "6   [228, 34, 485, 400]   25         -1            1  train    63278     w   \n",
            "\n",
            "    labels  \n",
            "1        0  \n",
            "2        0  \n",
            "4        0  \n",
            "7        0  \n",
            "0        1  \n",
            "..     ...  \n",
            "6       19  \n",
            "1       20  \n",
            "2       20  \n",
            "4       20  \n",
            "6       20  \n",
            "\n",
            "[84 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[1]['video_id'])\n",
        "metadata = df.iloc[1]\n",
        "label = metadata['labels']\n",
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwkukKbLvlqI",
        "outputId": "864f2256-681a-4f39-f09d-6ffc44ab695a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01610\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## TEST #############\n",
        "# Testing Image Snippet\n",
        "bbox = df.iloc[1]['bbox']\n",
        "fps = 30\n",
        "frame_end = df.iloc[1]['frame_end']\n",
        "frame_start = df.iloc[1]['frame_start']\n",
        "video_path = f\"/content/drive/MyDrive/videos/{df.iloc[1]['video_id']}.mp4\"\n",
        "frames = preprocess_videos(bbox, fps, frame_end, frame_start, video_path)\n",
        "\n",
        "delay = 1 / fps\n",
        "\n",
        "for frame in frames:\n",
        "  # Display the frame\n",
        "  cv2_imshow(frame)\n",
        "\n",
        "  # Wait for the specified delay or until a key is pressed\n",
        "  if cv2.waitKey(int(delay * 1000)) & 0xFF == ord('q'):  # Press 'q' to quit\n",
        "      break\n",
        "\n",
        "# Close the window after playback is finished\n",
        "cv2.destroyAllWindows()\n",
        "print(len(frames))"
      ],
      "metadata": {
        "id": "B6RWfdgJvZtg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "f8341c0d-84b5-4d07-ffec-2ce1003a8c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess_videos' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4399520068be>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mframe_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/drive/MyDrive/videos/{df.iloc[1]['video_id']}.mp4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_videos' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  ## No need for cuda:0 here as it only has one GPU and 0 is default\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O-w-FvoS72R",
        "outputId": "e18d2f88-3844-4b06-84d5-4b3512672649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT COMPUTING MEAN AND STD HERE"
      ],
      "metadata": {
        "id": "pLnyg3-7TJTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations to apply to each frame\n",
        "transformations = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),  # Scale to range [0, 1]\n",
        "    # v2.Normalize(mean=[mean], std=[std])  # Not doing mean and std for dataset\n",
        "])"
      ],
      "metadata": {
        "id": "hgaN5YDtX5G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_videos(bbox, fps, frame_end, frame_start, video_path):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "  duration_frames = 2 * 30  # 2 sec, 30 fps\n",
        "\n",
        "  if not cap.isOpened():\n",
        "    print(\"[INFO] Warning: Could not open video.\" + video_path)\n",
        "    return\n",
        "\n",
        "  frames = []\n",
        "  current_frame = 1\n",
        "\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    if (current_frame >= frame_start) and (frame_end == -1 or current_frame <= frame_end):\n",
        "      # Apply bounding box\n",
        "      x_min, y_min, x_max, y_max = bbox\n",
        "      cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "      # Resize frame\n",
        "      output_frame_size = (150, 150)  # Arbitrary\n",
        "      resized_frame = cv2.resize(cropped_frame, output_frame_size)  ## I can specify the interpolation method for better resizing such as BiCubic, Bilinear, etc.\n",
        "\n",
        "      # Randomly flipping, rotating frames\n",
        "      if random.random() > 0.5:\n",
        "        resized_frame = cv2.flip(resized_frame, 1)  # 1 means flipping around y-axis\n",
        "\n",
        "      # Randomly rotate the frame\n",
        "      rotation_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE]\n",
        "      rotation = random.choice(rotation_choices)\n",
        "      if rotation != 0:\n",
        "          resized_frame = cv2.rotate(resized_frame, rotation)\n",
        "\n",
        "      # Convert cv2 BGR to RGB format\n",
        "      resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      # No need for this step, because torchvision.transforms v2 itself will convert it.\n",
        "      # Transpose the frame from [height, width, channels] to [channels, height, width]\n",
        "      # resized_frame = np.transpose(resized_frame, (2, 0, 1))  #This format is PyTorch Native\n",
        "\n",
        "      # Tensor casting\n",
        "      tensor_frame = torch.from_numpy(resized_frame)  # Change numpy array to tensor # No need for .float() here as I am using transform above\n",
        "      frame_transformed = transformations(tensor_frame)  # Apply transformations\n",
        "\n",
        "      frames.append(frame_transformed)\n",
        "\n",
        "    current_frame += 1\n",
        "\n",
        "  cap.release()\n",
        "\n",
        "  if len(frames) < duration_frames:\n",
        "    # If fewer frames than desired, repeat the last frame\n",
        "    frames += [frames[-1]] * (duration_frames - len(frames))\n",
        "  elif len(frames) > duration_frames:\n",
        "    # If more frames than desired, sample evenly from the extracted frames\n",
        "    indices = torch.linspace(0, len(frames) - 1, duration_frames, dtype=torch.int)\n",
        "    frames = [frames[i] for i in indices]\n",
        "\n",
        "  return frames"
      ],
      "metadata": {
        "id": "_31H4GdP2E72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoSignLanguageDataset(Dataset):\n",
        "  def __init__(self, df, root):\n",
        "    self.data_frame = df\n",
        "    # self.transform = transform\n",
        "    self.root_path = root\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_frame)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    metadata = self.data_frame.iloc[idx]\n",
        "    bbox = metadata['bbox']\n",
        "    fps = metadata['fps']\n",
        "    frame_end = metadata['frame_end']\n",
        "    frame_start = metadata['frame_start']\n",
        "    split = metadata['split']\n",
        "    video_id = metadata['video_id']\n",
        "    gloss = metadata['gloss']\n",
        "    # label = self.data_frame.iloc[idx, -1]\n",
        "    label = metadata['labels']\n",
        "    video_path = f'{self.root_path}/{video_id}.mp4'\n",
        "\n",
        "    frames = preprocess_videos(bbox, fps, frame_end, frame_start, video_path)\n",
        "\n",
        "    # label = torch.tensor(label).long()  ## Both lines are almost same\n",
        "    label = torch.tensor(label, dtype=torch.int64)\n",
        "\n",
        "    if frames:\n",
        "      # Ensure all frames are of the same shape and tensor type before stacking\n",
        "      tensor_frames = torch.stack(frames)\n",
        "\n",
        "      # if self.transform:\n",
        "      #   frames = self.transform(frames)\n",
        "      # print(\"tf_type = \", tensor_frames.dtype)\n",
        "      # print(\"tf_size = \", tensor_frames.size())\n",
        "      return tensor_frames, label"
      ],
      "metadata": {
        "id": "9wo-6x2H5oWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom collate function to remove None values\n",
        "def custom_collate_fn(batch):\n",
        "  batch = list(filter(lambda x: x is not None, batch))  # Filter out None values\n",
        "\n",
        "  if not batch:\n",
        "    # Return None or a custom signal that indicates an empty batch\n",
        "    return None  ## This return None is not very good, I have to handle this better so it returns something default_collate() can work with.\n",
        "\n",
        "  return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "BATCH_SIZE = 12\n",
        "ROOT = \"/content/drive/MyDrive/videos\"\n",
        "training_dataset = VideoSignLanguageDataset(df=df, root=ROOT)\n",
        "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "OvEpRJxMTiL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_batch)\n",
        "unique_tensor = torch.unique(y_batch, return_counts=True)\n",
        "print(unique_tensor, len(unique_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CRY2PQNbzPo",
        "outputId": "148b5931-72e1-45f4-a6ce-b43455464ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1315, 1980, 1814, 1840, 1829, 1896, 1768, 1726, 1789, 1757, 1959, 1896,\n",
            "        1840, 1768, 1892, 1281, 1917, 1789, 1471, 1742, 1757, 1829, 1281, 1994,\n",
            "        1980, 1959, 1693, 1726, 1794, 1779, 1794, 1917, 1779, 1959, 1471, 1980,\n",
            "        1789, 1917, 1768, 1814, 1757, 1840, 1757, 1980, 1281, 1829, 1892, 1896,\n",
            "        1814, 1281, 1983, 1742, 1693, 1959, 1693, 1768, 1726, 1779, 1814, 1994,\n",
            "        1471, 1983, 1994, 1917, 1983, 1726, 1892, 1693, 1892, 1742, 1994, 1829,\n",
            "        1840, 1779, 1983, 1315, 1794, 1896, 1794, 1471, 1315, 1742, 1315, 1789])\n",
            "(tensor([1281, 1315, 1471, 1693, 1726, 1742, 1757, 1768, 1779, 1789, 1794, 1814,\n",
            "        1829, 1840, 1892, 1896, 1917, 1959, 1980, 1983, 1994]), tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])) 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in train_dataloader:\n",
        "  print(X.size())\n",
        "  print(len(X), len(y))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaWSZsUEOzPc",
        "outputId": "eeaa7b34-04b2-47fe-afe8-affe806f61a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 30, 3, 150, 150])\n",
            "12 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN + LSTM Model"
      ],
      "metadata": {
        "id": "qRddfeApdEwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLSTMNet(nn.Module):\n",
        "  def __init__(self, input_channels, output_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    self.resnet_cnn_model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
        "\n",
        "    # Replace the final fully connected layer of ResNet with an AdaptiveAvgPool2d\n",
        "    # This is because we only want to use ResNet as a feature extractor\n",
        "    self.resnet_cnn_model.fc = nn.Identity()\n",
        "    # The output size from ResNet will be [batch_size, 512, 1, 1]\n",
        "    # self.input_size = 512  # Based on ResNet\n",
        "\n",
        "    # # This snippet is for programatically determining the cnn output to serve as input for LSTM\n",
        "    dummy_input = torch.randn(1, input_channels, 150, 150)  ## [batch_size, channels, height, width]\n",
        "    dummy_output = self.resnet_cnn_model(dummy_input)\n",
        "    cnn_output = dummy_output.view(dummy_output.size(0), -1).size(1)\n",
        "\n",
        "    # LSTM Parameters\n",
        "    self.output_classes = output_classes  # number of output classes # 21 is num of alphabets available\n",
        "    self.num_layers = 2  # number of stacked layers of LSTM\n",
        "    #self.input_size = 512\n",
        "    self.input_size = cnn_output\n",
        "    self.hidden_size = 1024  # Arbitrary\n",
        "    # self.seq_length = 60  # Frames length ## Maybe use dummy_output.size(1)\n",
        "\n",
        "    self.LSTM = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
        "    # Maybe add few more dense layers to improve accuracy\n",
        "    self.fc_1 = nn.Linear(in_features=self.hidden_size, out_features=512)\n",
        "    self.relu_1 = nn.ReLU()\n",
        "    self.fc_2 = nn.Linear(in_features=512, out_features=256)\n",
        "    self.relu_2 = nn.ReLU()\n",
        "    self.fc_3 = nn.Linear(in_features=256, out_features=self.output_classes)\n",
        "    # No need for any final activation here if I'm using CrossEntropyLoss which internally handles 'Softmax' + 'NLLLoss' in single operation\n",
        "\n",
        "  def forward(self, x):\n",
        "    print(f\"Original x shape: {x.shape}\")  # Debugging\n",
        "    batch_size, seq_length, c, h, w = x.size()\n",
        "\n",
        "    # x = x.view(batch_size * seq_length, c, h, w)  ## Maybe better alternative\n",
        "    # x = x.view(-1, x.size(2), x.size(3), x.size(4))\n",
        "    x = x.view(-1, c, h, w) ## Converting 5D tensor to 4D as CNN expects\n",
        "    print(f\"Reshaped x for ResNet shape: {x.shape}\")  # Debugging\n",
        "    x = self.resnet_cnn_model(x)  ## Feeding into ResNet CNN\n",
        "    print(f\"Output from ResNet shape: {x.shape}\")  # Debugging\n",
        "\n",
        "    # Reshape CNN output for LSTM input\n",
        "    x = x.view(batch_size, seq_length, -1)  ## [batch_size, frames_of_video, features]\n",
        "    print(f\"Reshaped x for LSTM shape: {x.shape}\")  # Debugging\n",
        "    lstm_out, (hn, cn) = self.LSTM(x)  ## Feed into LSTM\n",
        "\n",
        "    # For many-to-one tasks, you might want to use hn[-1] or lstm_out[:, -1, :]\n",
        "    x = lstm_out[:, -1, :]\n",
        "\n",
        "    # Feed LSTM 'extracted' output to the following dense layers\n",
        "    x = self.fc_1(x)\n",
        "    x = self.relu_1(x)\n",
        "    x = self.fc_2(x)\n",
        "    x = self.relu_2(x)\n",
        "    x = self.fc_3(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "_n-pFiT0o5e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reproducible RNGs"
      ],
      "metadata": {
        "id": "m8X0AeNkdPiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set RNGs to same values every time including CUDA operations\n",
        "seed = 10\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "#torch.cuda.manual_seed_all(seed) ## For multiple GPUs"
      ],
      "metadata": {
        "id": "LlSiMWbadO1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & Testing"
      ],
      "metadata": {
        "id": "i4CRfeMRsCkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating instance of model\n",
        "input_channels = 3  # RGB\n",
        "output_classes = 21  # ASL # 21 is num of alphabets available in the dataset\n",
        "conv_lstm_model = ConvLSTMNet(input_channels, output_classes).to(DEVICE)\n",
        "print(conv_lstm_model)\n",
        "print(conv_lstm_model.summary())\n",
        "print(next(conv_lstm_model.parameters()).device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kONVEHYd67r",
        "outputId": "4fa2c514-e3e4-4d22-a34c-4291d0ef2617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvLSTMNet(\n",
            "  (resnet_cnn_model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "  )\n",
            "  (LSTM): LSTM(2048, 1024, num_layers=2, batch_first=True)\n",
            "  (fc_1): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (relu_1): ReLU()\n",
            "  (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (relu_2): ReLU()\n",
            "  (fc_3): Linear(in_features=256, out_features=21, bias=True)\n",
            ")\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and Optimizer\n",
        "scaler = GradScaler()\n",
        "LRN_RATE = 0.01\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "g_descent_optimizer = torch.optim.Adam(conv_lstm_model.parameters(), lr=LRN_RATE)  ##Adam is a type of gradient descent"
      ],
      "metadata": {
        "id": "FusGy-Oiszym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Model\n",
        "overall_train_losses = []\n",
        "overall_train_accuracy = []\n",
        "#test_losses = []\n",
        "#test_correct = []\n",
        "\n",
        "EPOCHS = 6\n",
        "#total_dataset = len(train_dataloader.dataset)\n",
        "\n",
        "print(\"[INFO] Training the network...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "  conv_lstm_model.train()  ## Set model to train mode\n",
        "\n",
        "  # These variables are for entire dataset once (all batches)\n",
        "  total_train_loss = 0\n",
        "  total_train_correct = 0\n",
        "\n",
        "  for X_batch, y_batch in train_dataloader:\n",
        "    #(X_batch[0].view(28, 28)\n",
        "    #print(X_batch.dim())  ## 4-Dimensions\n",
        "    #print(X_batch[0].dim())  ## 3-D\n",
        "\n",
        "    # Push Data Tensors to the GPU\n",
        "    (X_batch, y_batch) = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "\n",
        "    g_descent_optimizer.zero_grad()\n",
        "\n",
        "    with autocast():  ## Maybe no need to write arguments here; could just leave ()\n",
        "      pred = conv_lstm_model(X_batch)  ## Predicted values of y\n",
        "      loss = loss_function(pred, y_batch)\n",
        "\n",
        "    #g_descent_optimizer.zero_grad()\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    #loss.backward()\n",
        "    scaler.step(g_descent_optimizer)\n",
        "    #g_descent_optimizer.step()\n",
        "    scaler.update()\n",
        "\n",
        "    total_train_loss += loss.item()\n",
        "    total_train_correct += (pred.argmax(1) == y_batch).type(torch.float).sum().item()\n",
        "\n",
        "  ## For one epoch\n",
        "  avg_train_accuracy = total_train_correct / len(train_dataloader.dataset)\n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "  overall_train_accuracy.append(avg_train_accuracy)\n",
        "  overall_train_losses.append(avg_train_loss)\n",
        "\n",
        "  print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
        "  print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avg_train_loss, avg_train_accuracy))\n",
        "  print(f'{total_train_correct}/{len(train_dataloader.dataset)}')\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f'Total Training Time: {total_time/60} Minutes.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN-kzkl4tyh9",
        "outputId": "12384bed-292a-4e9d-a2a7-17d12e767ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training the network...\n",
            "Original x shape: torch.Size([12, 30, 3, 150, 150])\n",
            "Reshaped x for ResNet shape: torch.Size([360, 3, 150, 150])\n"
          ]
        }
      ]
    }
  ]
}